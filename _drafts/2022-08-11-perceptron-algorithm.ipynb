{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perceptron Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The perceptron is a **classifier**. It constructs a hyperplane called the **descion boundary** where all datapoints on one side of the descion boundary are in one class and all datapoints on the other side of the descion boundary are in the other class. More specifically, the perceptron is a **binary, linear, supervised classifier**. Binary because we are only classifying two classes here, linear because the hyperplane that forms the descion bounary is made up of straight lines, and supervised becasue we need the data to be labeled for this algorithm to work.\n",
    "\n",
    "Consider weight $w \\in \\mathbb{R}^d$, datapoint $x \\in \\mathbb{R}^d$, and intercept $b \\in \\mathbb{R}$. Then \n",
    "\\begin{align*}\n",
    "    H\n",
    "    &=\n",
    "    \\{x | w^T x + b = 0\\}\n",
    "\\end{align*}\n",
    "dennotes a hyperplane $H$ where $w$ is normal to the hyperplane. If $w^T x + b > 0$, we are on one side of the hyperplane. And if $w^T x + b < 0$, we are on another side of the hyperplane. The perceptron classifier is thus:\n",
    "\\begin{align*}\n",
    "    \\hat{y}\n",
    "    &=\n",
    "    sgn(w^T x + b)\n",
    "\\end{align*}\n",
    "where $sgn(a) = 1$ if $a \\geq 0$ else $-1$. In other words, if $w^T x + b \\geq 0$, we predict $\\hat{y}=1$ but if $w^T x + b < 0$, we predict $\\hat{y} = -1$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perceptron Learning Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**How do we learn the $w$ so that our perceptron classifier will allows make the correct predictions? Answer: the perceptron learning algorithm!**\n",
    "\n",
    "The perceptron algorithm goes like this:\n",
    "```\n",
    "w^(0)\n",
    "while \n",
    "```\n",
    "\n",
    "\n",
    "Formally, given a dataset $\\textbf{(X, Y)} = \\{(x_i, y_i)\\}_{i=1}^n \\subseteq \\mathbb{R}^d \\times \\{\\pm 1\\}$, the perceptron learning algorithm learns a weight $w$ such that\n",
    "\\begin{align*}\n",
    "    y_i\n",
    "    &=\n",
    "    \\hat{y}_i\n",
    "\\end{align*}\n",
    "for all $i=1, ..., n$. In other words, the perceptron algorithm will learn a $w$ that will able to correctly classify *all* datapoints. The algorithm is as follows:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Theorem** Consider the dataset $(x_i, y_i)$ where $x_i \\in \\mathbb{R}^d$ and $y_i \\in {-1, 1}$ for all $i=1, ..., n$. If there exists some weight $w^* \\in \\mathbb{R}^d$ such that $||w||_2 = 1$, some radius $R \\geq ||x_i||_2$ for all $i=1, ..., n$, and some margin $\\gamma > 0$ such that\n",
    "$$\n",
    "y_i (x_i \\cdot w^*) \\geq \\gamma\n",
    "$$\n",
    "then the perceptron algorithm makes at most \n",
    "$$\n",
    "\\frac{R^2}{\\gamma^2}\n",
    "$$\n",
    "mistakes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\tag*{$\\blacksquare$}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\require{\\mhchem}\n",
    "\\ce{H2O}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
